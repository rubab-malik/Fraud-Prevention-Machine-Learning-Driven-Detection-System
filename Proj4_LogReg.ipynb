{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b309c568",
   "metadata": {},
   "source": [
    "# Fraud Detection System for Online Transactions\n",
    "Project 4: Machine Learning Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704e0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2cb71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "file_path1 = Path(\"Resources/test_identity.csv\")\n",
    "file_path2 = Path(\"Resources/test_transaction.csv\")\n",
    "file_path3 = Path(\"Resources/train_identity.csv\")\n",
    "file_path4 = Path(\"Resources/train_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad44bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "\n",
    "test_identity = pd.read_csv(file_path1)\n",
    "test_transaction = pd.read_csv(file_path2)\n",
    "train_identity = pd.read_csv(file_path3)\n",
    "train_transaction = pd.read_csv(file_path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
<<<<<<< HEAD
   "id": "22d6b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TBC: Find column names.\n",
    "# Load in data\n",
    "\n",
    "# columns = [\n",
    "#     'age',\n",
    "#     'workclass',\n",
    "#     'fnlwgt',\n",
    "#     'education',\n",
    "#     'education_num',\n",
    "#     'occupation',\n",
    "#     'relationship',\n",
    "#     'capital-gain',\n",
    "#     'capital-loss',\n",
    "#     'hours-per-week',\n",
    "#     'native-country',\n",
    "#     'income'\n",
    "    \n",
    "# ]\n",
    "\n",
    "#test_transaction2 = pd.read_csv('../Resources/data.csv', header=None, names=columns, index_col=False)\n",
    "# test_transaction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b04d4a",
=======
   "id": "7b77775d",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>id-01</th>\n",
       "      <th>id-02</th>\n",
       "      <th>id-03</th>\n",
       "      <th>id-04</th>\n",
       "      <th>id-05</th>\n",
       "      <th>id-06</th>\n",
       "      <th>id-07</th>\n",
       "      <th>id-08</th>\n",
       "      <th>id-09</th>\n",
       "      <th>...</th>\n",
       "      <th>id-31</th>\n",
       "      <th>id-32</th>\n",
       "      <th>id-33</th>\n",
       "      <th>id-34</th>\n",
       "      <th>id-35</th>\n",
       "      <th>id-36</th>\n",
       "      <th>id-37</th>\n",
       "      <th>id-38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663586</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>280290.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>chrome 67.0 for android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>mobile</td>\n",
       "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>chrome 67.0 for android</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1280x720</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>LGLS676 Build/MXB48T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663597</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>185210.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ie 11.0 for tablet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Trident/7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663601</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>252944.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>chrome 67.0 for android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>mobile</td>\n",
       "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663602</td>\n",
       "      <td>-95.0</td>\n",
       "      <td>328680.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>chrome 67.0 for android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SM-G9650 Build/R16NW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  id-01     id-02  id-03  id-04  id-05  id-06  id-07  id-08  \\\n",
       "0        3663586  -45.0  280290.0    NaN    NaN    0.0    0.0    NaN    NaN   \n",
       "1        3663588    0.0    3579.0    0.0    0.0    0.0    0.0    NaN    NaN   \n",
       "2        3663597   -5.0  185210.0    NaN    NaN    1.0    0.0    NaN    NaN   \n",
       "3        3663601  -45.0  252944.0    0.0    0.0    0.0    0.0    NaN    NaN   \n",
       "4        3663602  -95.0  328680.0    NaN    NaN    7.0  -33.0    NaN    NaN   \n",
       "\n",
       "   id-09  ...                    id-31  id-32     id-33           id-34  \\\n",
       "0    NaN  ...  chrome 67.0 for android    NaN       NaN             NaN   \n",
       "1    0.0  ...  chrome 67.0 for android   24.0  1280x720  match_status:2   \n",
       "2    NaN  ...       ie 11.0 for tablet    NaN       NaN             NaN   \n",
       "3    0.0  ...  chrome 67.0 for android    NaN       NaN             NaN   \n",
       "4    NaN  ...  chrome 67.0 for android    NaN       NaN             NaN   \n",
       "\n",
       "   id-35 id-36 id-37  id-38  DeviceType                   DeviceInfo  \n",
       "0      F     F     T      F      mobile  MYA-L13 Build/HUAWEIMYA-L13  \n",
       "1      T     F     T      T      mobile         LGLS676 Build/MXB48T  \n",
       "2      F     T     T      F     desktop                  Trident/7.0  \n",
       "3      F     F     T      F      mobile  MYA-L13 Build/HUAWEIMYA-L13  \n",
       "4      F     F     T      F      mobile         SM-G9650 Build/R16NW  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_identity.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "id": "dcd04a96",
=======
   "execution_count": 5,
   "id": "86bfb169",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>V330</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>18403224</td>\n",
       "      <td>31.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10409</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>18403263</td>\n",
       "      <td>49.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4272</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>18403310</td>\n",
       "      <td>171.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4476</td>\n",
       "      <td>574.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>18403310</td>\n",
       "      <td>284.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10989</td>\n",
       "      <td>360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>18403317</td>\n",
       "      <td>67.95</td>\n",
       "      <td>W</td>\n",
       "      <td>18018</td>\n",
       "      <td>452.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 393 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
       "0        3663549       18403224           31.95         W  10409  111.0   \n",
       "1        3663550       18403263           49.00         W   4272  111.0   \n",
       "2        3663551       18403310          171.00         W   4476  574.0   \n",
       "3        3663552       18403310          284.95         W  10989  360.0   \n",
       "4        3663553       18403317           67.95         W  18018  452.0   \n",
       "\n",
       "   card3       card4  card5  card6  ...  V330  V331  V332  V333 V334 V335  \\\n",
       "0  150.0        visa  226.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "1  150.0        visa  226.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "2  150.0        visa  226.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "3  150.0        visa  166.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "4  150.0  mastercard  117.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "\n",
       "   V336  V337  V338  V339  \n",
       "0   NaN   NaN   NaN   NaN  \n",
       "1   NaN   NaN   NaN   NaN  \n",
       "2   NaN   NaN   NaN   NaN  \n",
       "3   NaN   NaN   NaN   NaN  \n",
       "4   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 393 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed642c",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Examine the characteristics of each dataset.\n",
    "Remove irrelevant data, missing values, or duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49435e1",
   "metadata": {},
   "source": [
    "### Check data types"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "id": "a947ff62",
=======
   "execution_count": 6,
   "id": "7059d0b7",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID      int64\n",
       "id-01            float64\n",
       "id-02            float64\n",
       "id-03            float64\n",
       "id-04            float64\n",
       "id-05            float64\n",
       "id-06            float64\n",
       "id-07            float64\n",
       "id-08            float64\n",
       "id-09            float64\n",
       "id-10            float64\n",
       "id-11            float64\n",
       "id-12             object\n",
       "id-13            float64\n",
       "id-14            float64\n",
       "id-15             object\n",
       "id-16             object\n",
       "id-17            float64\n",
       "id-18            float64\n",
       "id-19            float64\n",
       "id-20            float64\n",
       "id-21            float64\n",
       "id-22            float64\n",
       "id-23             object\n",
       "id-24            float64\n",
       "id-25            float64\n",
       "id-26            float64\n",
       "id-27             object\n",
       "id-28             object\n",
       "id-29             object\n",
       "id-30             object\n",
       "id-31             object\n",
       "id-32            float64\n",
       "id-33             object\n",
       "id-34             object\n",
       "id-35             object\n",
       "id-36             object\n",
       "id-37             object\n",
       "id-38             object\n",
       "DeviceType        object\n",
       "DeviceInfo        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_identity.dtypes "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "id": "0ad9af36",
=======
   "execution_count": 7,
   "id": "0f4a49f9",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID       int64\n",
       "TransactionDT       int64\n",
       "TransactionAmt    float64\n",
       "ProductCD          object\n",
       "card1               int64\n",
       "                   ...   \n",
       "V335              float64\n",
       "V336              float64\n",
       "V337              float64\n",
       "V338              float64\n",
       "V339              float64\n",
       "Length: 393, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transaction.dtypes"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "id": "d098439a",
=======
   "execution_count": 8,
   "id": "a28d37b5",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']\n"
     ]
    }
   ],
   "source": [
    "column_test_transaction = test_transaction.columns.tolist()\n",
    "print(column_test_transaction)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "id": "c24bc083",
=======
   "execution_count": 9,
   "id": "7eeb1f0f",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506691\n"
     ]
    }
   ],
   "source": [
    "# count the number of rows\n",
    "test_transaction_rows = len(test_transaction)\n",
    "\n",
    "# print the result\n",
    "print(test_transaction_rows)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "id": "905c2a62",
=======
   "execution_count": 10,
   "id": "8c35b227",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID      int64\n",
       "id_01            float64\n",
       "id_02            float64\n",
       "id_03            float64\n",
       "id_04            float64\n",
       "id_05            float64\n",
       "id_06            float64\n",
       "id_07            float64\n",
       "id_08            float64\n",
       "id_09            float64\n",
       "id_10            float64\n",
       "id_11            float64\n",
       "id_12             object\n",
       "id_13            float64\n",
       "id_14            float64\n",
       "id_15             object\n",
       "id_16             object\n",
       "id_17            float64\n",
       "id_18            float64\n",
       "id_19            float64\n",
       "id_20            float64\n",
       "id_21            float64\n",
       "id_22            float64\n",
       "id_23             object\n",
       "id_24            float64\n",
       "id_25            float64\n",
       "id_26            float64\n",
       "id_27             object\n",
       "id_28             object\n",
       "id_29             object\n",
       "id_30             object\n",
       "id_31             object\n",
       "id_32            float64\n",
       "id_33             object\n",
       "id_34             object\n",
       "id_35             object\n",
       "id_36             object\n",
       "id_37             object\n",
       "id_38             object\n",
       "DeviceType        object\n",
       "DeviceInfo        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_identity.dtypes"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "id": "39919527",
=======
   "execution_count": 11,
   "id": "ad8168bc",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID       int64\n",
       "isFraud             int64\n",
       "TransactionDT       int64\n",
       "TransactionAmt    float64\n",
       "ProductCD          object\n",
       "                   ...   \n",
       "V335              float64\n",
       "V336              float64\n",
       "V337              float64\n",
       "V338              float64\n",
       "V339              float64\n",
       "Length: 394, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transaction.dtypes"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "id": "4536fc34",
=======
   "execution_count": 12,
   "id": "fe2a49b4",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']\n"
     ]
    }
   ],
   "source": [
    "column_train_transaction = train_transaction.columns.tolist()\n",
    "print(column_train_transaction)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "id": "bacbf2dc",
=======
   "execution_count": 13,
   "id": "8cf38fab",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590540\n"
     ]
    }
   ],
   "source": [
    "# count the number of rows\n",
    "train_transaction_rows = len(train_transaction)\n",
    "\n",
    "# print the result\n",
    "print(train_transaction_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b176162",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "id": "a48c17a2",
=======
   "execution_count": 14,
   "id": "7c301c91",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionID    141907\n",
      "id-01            141907\n",
      "id-02            136976\n",
      "id-03             66481\n",
      "id-04             66481\n",
      "id-05            134750\n",
      "id-06            134750\n",
      "id-07              5059\n",
      "id-08              5059\n",
      "id-09             74338\n",
      "id-10             74338\n",
      "id-11            136778\n",
      "id-12            141907\n",
      "id-13            130286\n",
      "id-14             71357\n",
      "id-15            136977\n",
      "id-16            125747\n",
      "id-17            135966\n",
      "id-18             50875\n",
      "id-19            135906\n",
      "id-20            135633\n",
      "id-21              5059\n",
      "id-22              5062\n",
      "id-23              5062\n",
      "id-24              4740\n",
      "id-25              5039\n",
      "id-26              5047\n",
      "id-27              5062\n",
      "id-28            136778\n",
      "id-29            136778\n",
      "id-30             70659\n",
      "id-31            136625\n",
      "id-32             70671\n",
      "id-33             70671\n",
      "id-34             72175\n",
      "id-35            136977\n",
      "id-36            136977\n",
      "id-37            136977\n",
      "id-38            136977\n",
      "DeviceType       136931\n",
      "DeviceInfo       115057\n",
      "dtype: int64\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## for test_identity\n",
    "print(test_identity.count())\n",
    "print(test_identity.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "id": "92529394",
=======
   "execution_count": 15,
   "id": "3bbaecf3",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionID     506691\n",
      "TransactionDT     506691\n",
      "TransactionAmt    506691\n",
      "ProductCD         506691\n",
      "card1             506691\n",
      "                   ...  \n",
      "V335               76431\n",
      "V336               76431\n",
      "V337               76431\n",
      "V338               76431\n",
      "V339               76431\n",
      "Length: 393, dtype: int64\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## for test_transaction\n",
    "print(test_transaction.count())\n",
    "print(test_transaction.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "id": "e0b2bcb1",
=======
   "execution_count": 16,
   "id": "2f450cd5",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionID    144233\n",
      "id_01            144233\n",
      "id_02            140872\n",
      "id_03             66324\n",
      "id_04             66324\n",
      "id_05            136865\n",
      "id_06            136865\n",
      "id_07              5155\n",
      "id_08              5155\n",
      "id_09             74926\n",
      "id_10             74926\n",
      "id_11            140978\n",
      "id_12            144233\n",
      "id_13            127320\n",
      "id_14             80044\n",
      "id_15            140985\n",
      "id_16            129340\n",
      "id_17            139369\n",
      "id_18             45113\n",
      "id_19            139318\n",
      "id_20            139261\n",
      "id_21              5159\n",
      "id_22              5169\n",
      "id_23              5169\n",
      "id_24              4747\n",
      "id_25              5132\n",
      "id_26              5163\n",
      "id_27              5169\n",
      "id_28            140978\n",
      "id_29            140978\n",
      "id_30             77565\n",
      "id_31            140282\n",
      "id_32             77586\n",
      "id_33             73289\n",
      "id_34             77805\n",
      "id_35            140985\n",
      "id_36            140985\n",
      "id_37            140985\n",
      "id_38            140985\n",
      "DeviceType       140810\n",
      "DeviceInfo       118666\n",
      "dtype: int64\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## for train_identity\n",
    "print(train_identity.count())\n",
    "print(train_identity.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "id": "525de8b4",
=======
   "execution_count": 17,
   "id": "ae10daa4",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionID     590540\n",
      "isFraud           590540\n",
      "TransactionDT     590540\n",
      "TransactionAmt    590540\n",
      "ProductCD         590540\n",
      "                   ...  \n",
      "V335               82351\n",
      "V336               82351\n",
      "V337               82351\n",
      "V338               82351\n",
      "V339               82351\n",
      "Length: 394, dtype: int64\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## for train_transaction\n",
    "print(train_transaction.count())\n",
    "print(train_transaction.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb1eb8",
   "metadata": {},
   "source": [
    "#### Any missing data? yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee9b3f0",
   "metadata": {},
   "source": [
    "### Check for duplicate entries"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "id": "0c9268c0",
=======
   "execution_count": 18,
   "id": "a4954d00",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_identity.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00f554cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transaction.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
<<<<<<< HEAD
   "id": "7bdf6417",
=======
   "id": "2656296d",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "test_transaction.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff93ebb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
=======
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
    "train_identity.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "id": "27ab7161",
=======
   "execution_count": 21,
   "id": "e5530f2e",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transaction.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0029c",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "id": "9ccbddb2",
=======
   "execution_count": 22,
   "id": "ea4bed7e",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144233\n",
      "141907\n"
     ]
    }
   ],
   "source": [
    "# Check if all Transactions IDs from transaction dataset are in identity dataset.\n",
    "\n",
    "print(np.sum(train_transaction['TransactionID'].isin(train_identity['TransactionID'].unique())))\n",
    "print(np.sum(test_transaction['TransactionID'].isin(test_identity['TransactionID'].unique())))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
   "id": "18f5581a",
=======
   "execution_count": 23,
   "id": "9b813978",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge both the transaction and identity by left\n",
    "train_df=pd.merge(train_transaction,train_identity,how=\"left\",on=\"TransactionID\")\n",
    "test_df=pd.merge(test_transaction,test_identity,how=\"left\",on=\"TransactionID\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
   "id": "02e5b192",
=======
   "execution_count": 24,
   "id": "ad6c1fc6",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset shape:  (590540, 434)\n",
      "Test Dataset shape:  (506691, 433)\n"
     ]
    }
   ],
   "source": [
    "# Print Shapes\n",
    "print(\"Train Dataset shape: \", train_df.shape)\n",
    "print(\"Test Dataset shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
   "id": "f3f9221e",
=======
   "execution_count": 25,
   "id": "b2bd81ab",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2220x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5  ...                id_31  id_32  \\\n",
       "0    NaN  150.0    discover  142.0  ...                  NaN    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  ...                  NaN    NaN   \n",
       "2  490.0  150.0        visa  166.0  ...                  NaN    NaN   \n",
       "3  567.0  150.0  mastercard  117.0  ...                  NaN    NaN   \n",
       "4  514.0  150.0  mastercard  102.0  ...  samsung browser 6.2   32.0   \n",
       "\n",
       "       id_33           id_34  id_35 id_36 id_37  id_38  DeviceType  \\\n",
       "0        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "1        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "2        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "3        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "4  2220x1080  match_status:2      T     F     T      T      mobile   \n",
       "\n",
       "                      DeviceInfo  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4  SAMSUNG SM-G892A Build/NRD90M  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "id": "473ffb77",
=======
   "execution_count": 26,
   "id": "0787bf43",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>id-31</th>\n",
       "      <th>id-32</th>\n",
       "      <th>id-33</th>\n",
       "      <th>id-34</th>\n",
       "      <th>id-35</th>\n",
       "      <th>id-36</th>\n",
       "      <th>id-37</th>\n",
       "      <th>id-38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>18403224</td>\n",
       "      <td>31.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10409</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>18403263</td>\n",
       "      <td>49.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4272</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>18403310</td>\n",
       "      <td>171.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4476</td>\n",
       "      <td>574.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>18403310</td>\n",
       "      <td>284.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10989</td>\n",
       "      <td>360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>18403317</td>\n",
       "      <td>67.95</td>\n",
       "      <td>W</td>\n",
       "      <td>18018</td>\n",
       "      <td>452.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 433 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
       "0        3663549       18403224           31.95         W  10409  111.0   \n",
       "1        3663550       18403263           49.00         W   4272  111.0   \n",
       "2        3663551       18403310          171.00         W   4476  574.0   \n",
       "3        3663552       18403310          284.95         W  10989  360.0   \n",
       "4        3663553       18403317           67.95         W  18018  452.0   \n",
       "\n",
       "   card3       card4  card5  card6  ...  id-31  id-32  id-33  id-34 id-35  \\\n",
       "0  150.0        visa  226.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "1  150.0        visa  226.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "2  150.0        visa  226.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "3  150.0        visa  166.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "4  150.0  mastercard  117.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "\n",
       "  id-36  id-37  id-38  DeviceType  DeviceInfo  \n",
       "0   NaN    NaN    NaN         NaN         NaN  \n",
       "1   NaN    NaN    NaN         NaN         NaN  \n",
       "2   NaN    NaN    NaN         NaN         NaN  \n",
       "3   NaN    NaN    NaN         NaN         NaN  \n",
       "4   NaN    NaN    NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 433 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da94b5e",
   "metadata": {},
   "source": [
    "#### Examine summary statistics"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "id": "78f1dc22",
=======
   "execution_count": 27,
   "id": "e827684d",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <td>590540.0</td>\n",
       "      <td>3.282270e+06</td>\n",
       "      <td>1.704744e+05</td>\n",
       "      <td>2987000.000</td>\n",
       "      <td>3134634.750</td>\n",
       "      <td>3282269.500</td>\n",
       "      <td>3429904.25</td>\n",
       "      <td>3.577539e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isFraud</th>\n",
       "      <td>590540.0</td>\n",
       "      <td>3.499001e-02</td>\n",
       "      <td>1.837546e-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionDT</th>\n",
       "      <td>590540.0</td>\n",
       "      <td>7.372311e+06</td>\n",
       "      <td>4.617224e+06</td>\n",
       "      <td>86400.000</td>\n",
       "      <td>3027057.750</td>\n",
       "      <td>7306527.500</td>\n",
       "      <td>11246620.00</td>\n",
       "      <td>1.581113e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionAmt</th>\n",
       "      <td>590540.0</td>\n",
       "      <td>1.350272e+02</td>\n",
       "      <td>2.391625e+02</td>\n",
       "      <td>0.251</td>\n",
       "      <td>43.321</td>\n",
       "      <td>68.769</td>\n",
       "      <td>125.00</td>\n",
       "      <td>3.193739e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card1</th>\n",
       "      <td>590540.0</td>\n",
       "      <td>9.898735e+03</td>\n",
       "      <td>4.901170e+03</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>6019.000</td>\n",
       "      <td>9678.000</td>\n",
       "      <td>14184.00</td>\n",
       "      <td>1.839600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_22</th>\n",
       "      <td>5169.0</td>\n",
       "      <td>1.600271e+01</td>\n",
       "      <td>6.897665e+00</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>4.400000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_24</th>\n",
       "      <td>4747.0</td>\n",
       "      <td>1.280093e+01</td>\n",
       "      <td>2.372447e+00</td>\n",
       "      <td>11.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.600000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_25</th>\n",
       "      <td>5132.0</td>\n",
       "      <td>3.296089e+02</td>\n",
       "      <td>9.746109e+01</td>\n",
       "      <td>100.000</td>\n",
       "      <td>321.000</td>\n",
       "      <td>321.000</td>\n",
       "      <td>371.00</td>\n",
       "      <td>5.480000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_26</th>\n",
       "      <td>5163.0</td>\n",
       "      <td>1.490703e+02</td>\n",
       "      <td>3.210199e+01</td>\n",
       "      <td>100.000</td>\n",
       "      <td>119.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>169.00</td>\n",
       "      <td>2.160000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_32</th>\n",
       "      <td>77586.0</td>\n",
       "      <td>2.650860e+01</td>\n",
       "      <td>3.737502e+00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>32.00</td>\n",
       "      <td>3.200000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count          mean           std          min  \\\n",
       "TransactionID   590540.0  3.282270e+06  1.704744e+05  2987000.000   \n",
       "isFraud         590540.0  3.499001e-02  1.837546e-01        0.000   \n",
       "TransactionDT   590540.0  7.372311e+06  4.617224e+06    86400.000   \n",
       "TransactionAmt  590540.0  1.350272e+02  2.391625e+02        0.251   \n",
       "card1           590540.0  9.898735e+03  4.901170e+03     1000.000   \n",
       "...                  ...           ...           ...          ...   \n",
       "id_22             5169.0  1.600271e+01  6.897665e+00       10.000   \n",
       "id_24             4747.0  1.280093e+01  2.372447e+00       11.000   \n",
       "id_25             5132.0  3.296089e+02  9.746109e+01      100.000   \n",
       "id_26             5163.0  1.490703e+02  3.210199e+01      100.000   \n",
       "id_32            77586.0  2.650860e+01  3.737502e+00        0.000   \n",
       "\n",
       "                        25%          50%          75%           max  \n",
       "TransactionID   3134634.750  3282269.500   3429904.25  3.577539e+06  \n",
       "isFraud               0.000        0.000         0.00  1.000000e+00  \n",
       "TransactionDT   3027057.750  7306527.500  11246620.00  1.581113e+07  \n",
       "TransactionAmt       43.321       68.769       125.00  3.193739e+04  \n",
       "card1              6019.000     9678.000     14184.00  1.839600e+04  \n",
       "...                     ...          ...          ...           ...  \n",
       "id_22                14.000       14.000        14.00  4.400000e+01  \n",
       "id_24                11.000       11.000        15.00  2.600000e+01  \n",
       "id_25               321.000      321.000       371.00  5.480000e+02  \n",
       "id_26               119.000      149.000       169.00  2.160000e+02  \n",
       "id_32                24.000       24.000        32.00  3.200000e+01  \n",
       "\n",
       "[403 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
   "id": "e4dbef03",
=======
   "execution_count": 28,
   "id": "78454021",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <td>506691.0</td>\n",
       "      <td>3.916894e+06</td>\n",
       "      <td>1.462692e+05</td>\n",
       "      <td>3.663549e+06</td>\n",
       "      <td>3790221.5</td>\n",
       "      <td>3916894.00</td>\n",
       "      <td>4043566.5</td>\n",
       "      <td>4170239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionDT</th>\n",
       "      <td>506691.0</td>\n",
       "      <td>2.692994e+07</td>\n",
       "      <td>4.756507e+06</td>\n",
       "      <td>1.840322e+07</td>\n",
       "      <td>22771540.5</td>\n",
       "      <td>27204658.00</td>\n",
       "      <td>31348560.5</td>\n",
       "      <td>34214345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionAmt</th>\n",
       "      <td>506691.0</td>\n",
       "      <td>1.347256e+02</td>\n",
       "      <td>2.457798e+02</td>\n",
       "      <td>1.800000e-02</td>\n",
       "      <td>40.0</td>\n",
       "      <td>67.95</td>\n",
       "      <td>125.0</td>\n",
       "      <td>10270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card1</th>\n",
       "      <td>506691.0</td>\n",
       "      <td>9.957222e+03</td>\n",
       "      <td>4.884961e+03</td>\n",
       "      <td>1.001000e+03</td>\n",
       "      <td>6019.0</td>\n",
       "      <td>9803.00</td>\n",
       "      <td>14276.0</td>\n",
       "      <td>18397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card2</th>\n",
       "      <td>498037.0</td>\n",
       "      <td>3.637354e+02</td>\n",
       "      <td>1.586887e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>207.0</td>\n",
       "      <td>369.00</td>\n",
       "      <td>512.0</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id-22</th>\n",
       "      <td>5062.0</td>\n",
       "      <td>1.533682e+01</td>\n",
       "      <td>5.618032e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id-24</th>\n",
       "      <td>4740.0</td>\n",
       "      <td>1.316667e+01</td>\n",
       "      <td>3.222440e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id-25</th>\n",
       "      <td>5039.0</td>\n",
       "      <td>3.320431e+02</td>\n",
       "      <td>8.635668e+01</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>321.0</td>\n",
       "      <td>321.00</td>\n",
       "      <td>355.0</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id-26</th>\n",
       "      <td>5047.0</td>\n",
       "      <td>1.527529e+02</td>\n",
       "      <td>3.191700e+01</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>137.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>182.0</td>\n",
       "      <td>216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id-32</th>\n",
       "      <td>70671.0</td>\n",
       "      <td>2.621794e+01</td>\n",
       "      <td>3.601046e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.00</td>\n",
       "      <td>32.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count          mean           std           min  \\\n",
       "TransactionID   506691.0  3.916894e+06  1.462692e+05  3.663549e+06   \n",
       "TransactionDT   506691.0  2.692994e+07  4.756507e+06  1.840322e+07   \n",
       "TransactionAmt  506691.0  1.347256e+02  2.457798e+02  1.800000e-02   \n",
       "card1           506691.0  9.957222e+03  4.884961e+03  1.001000e+03   \n",
       "card2           498037.0  3.637354e+02  1.586887e+02  1.000000e+02   \n",
       "...                  ...           ...           ...           ...   \n",
       "id-22             5062.0  1.533682e+01  5.618032e+00  1.100000e+01   \n",
       "id-24             4740.0  1.316667e+01  3.222440e+00  1.000000e+01   \n",
       "id-25             5039.0  3.320431e+02  8.635668e+01  1.000000e+02   \n",
       "id-26             5047.0  1.527529e+02  3.191700e+01  1.000000e+02   \n",
       "id-32            70671.0  2.621794e+01  3.601046e+00  8.000000e+00   \n",
       "\n",
       "                       25%          50%         75%         max  \n",
       "TransactionID    3790221.5   3916894.00   4043566.5   4170239.0  \n",
       "TransactionDT   22771540.5  27204658.00  31348560.5  34214345.0  \n",
       "TransactionAmt        40.0        67.95       125.0     10270.0  \n",
       "card1               6019.0      9803.00     14276.0     18397.0  \n",
       "card2                207.0       369.00       512.0       600.0  \n",
       "...                    ...          ...         ...         ...  \n",
       "id-22                 14.0        14.00        14.0        44.0  \n",
       "id-24                 11.0        11.00        15.0        26.0  \n",
       "id-25                321.0       321.00       355.0       549.0  \n",
       "id-26                137.0       147.00       182.0       216.0  \n",
       "id-32                 24.0        24.00        32.0        48.0  \n",
       "\n",
       "[402 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61d1eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 414 columns in train dataset with missing values.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {train_df.isnull().any().sum()} columns in train dataset with missing values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8390d729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 385 columns in test dataset with missing values.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {test_df.isnull().any().sum()} columns in test dataset with missing values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5444e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in train and test data\n",
    "missing_train = train_df.isnull().sum().sort_values(ascending=False)\n",
    "missing_test = test_df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e337bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train (%):\n",
      "id_24            99.196159\n",
      "id_25            99.130965\n",
      "id_07            99.127070\n",
      "id_08            99.127070\n",
      "id_21            99.126393\n",
      "                   ...    \n",
      "C3                0.000000\n",
      "C2                0.000000\n",
      "C1                0.000000\n",
      "isFraud           0.000000\n",
      "TransactionID     0.000000\n",
      "Length: 434, dtype: float64\n",
      "-------------------------------------\n",
      "\n",
      "Missing values in test_data (%):\n",
      "id-24            99.064519\n",
      "id-25            99.005508\n",
      "id-26            99.003929\n",
      "id-21            99.001561\n",
      "id-08            99.001561\n",
      "                   ...    \n",
      "V113              0.000000\n",
      "V114              0.000000\n",
      "V115              0.000000\n",
      "V116              0.000000\n",
      "TransactionID     0.000000\n",
      "Length: 433, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Display the percentage of missing values in each column\n",
    "print(\"Missing values in train (%):\")\n",
    "print((missing_train / len(train_df)) * 100)\n",
    "print('-------------------------------------')\n",
    "print(\"\\nMissing values in test_data (%):\")\n",
    "print((missing_test / len(test_df)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f57d6c",
   "metadata": {},
   "source": [
    "### Address Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf6fca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 50% missing values\n",
    "train_data = train_df.drop(columns=missing_train[missing_train > 0.5 * len(train_df)].index)\n",
    "test_data = test_df.drop(columns=missing_test[missing_test > 0.5 * len(test_df)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07a0d7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shann\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \n",
      "C:\\Users\\shann\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values in the remaining columns with their respective means\n",
    "train_data.fillna(train_data.mean(), inplace=True)\n",
    "test_data.fillna(test_data.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a208ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data to new CSV files\n",
    "train_data.to_csv(\"Resources/clean_train_data.csv\", index=False)\n",
    "test_data.to_csv(\"Resources/clean_test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12019241",
   "metadata": {},
   "source": [
    "##### Consider adding visualisations such as correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5833df",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76637799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "clean_train_data = pd.read_csv(\"Resources/clean_train_data.csv\")\n",
    "clean_test_data = pd.read_csv(\"Resources/clean_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d73acb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionID: int64\n",
      "isFraud: int64\n",
      "TransactionDT: int64\n",
      "TransactionAmt: float64\n",
      "ProductCD: object\n",
      "card1: int64\n",
      "card2: float64\n",
      "card3: float64\n",
      "card4: object\n",
      "card5: float64\n",
      "card6: object\n",
      "addr1: float64\n",
      "addr2: float64\n",
      "P_emaildomain: object\n",
      "C1: float64\n",
      "C2: float64\n",
      "C3: float64\n",
      "C4: float64\n",
      "C5: float64\n",
      "C6: float64\n",
      "C7: float64\n",
      "C8: float64\n",
      "C9: float64\n",
      "C10: float64\n",
      "C11: float64\n",
      "C12: float64\n",
      "C13: float64\n",
      "C14: float64\n",
      "D1: float64\n",
      "D2: float64\n",
      "D3: float64\n",
      "D4: float64\n",
      "D10: float64\n",
      "D11: float64\n",
      "D15: float64\n",
      "M1: object\n",
      "M2: object\n",
      "M3: object\n",
      "M4: object\n",
      "M6: object\n",
      "V1: float64\n",
      "V2: float64\n",
      "V3: float64\n",
      "V4: float64\n",
      "V5: float64\n",
      "V6: float64\n",
      "V7: float64\n",
      "V8: float64\n",
      "V9: float64\n",
      "V10: float64\n",
      "V11: float64\n",
      "V12: float64\n",
      "V13: float64\n",
      "V14: float64\n",
      "V15: float64\n",
      "V16: float64\n",
      "V17: float64\n",
      "V18: float64\n",
      "V19: float64\n",
      "V20: float64\n",
      "V21: float64\n",
      "V22: float64\n",
      "V23: float64\n",
      "V24: float64\n",
      "V25: float64\n",
      "V26: float64\n",
      "V27: float64\n",
      "V28: float64\n",
      "V29: float64\n",
      "V30: float64\n",
      "V31: float64\n",
      "V32: float64\n",
      "V33: float64\n",
      "V34: float64\n",
      "V35: float64\n",
      "V36: float64\n",
      "V37: float64\n",
      "V38: float64\n",
      "V39: float64\n",
      "V40: float64\n",
      "V41: float64\n",
      "V42: float64\n",
      "V43: float64\n",
      "V44: float64\n",
      "V45: float64\n",
      "V46: float64\n",
      "V47: float64\n",
      "V48: float64\n",
      "V49: float64\n",
      "V50: float64\n",
      "V51: float64\n",
      "V52: float64\n",
      "V53: float64\n",
      "V54: float64\n",
      "V55: float64\n",
      "V56: float64\n",
      "V57: float64\n",
      "V58: float64\n",
      "V59: float64\n",
      "V60: float64\n",
      "V61: float64\n",
      "V62: float64\n",
      "V63: float64\n",
      "V64: float64\n",
      "V65: float64\n",
      "V66: float64\n",
      "V67: float64\n",
      "V68: float64\n",
      "V69: float64\n",
      "V70: float64\n",
      "V71: float64\n",
      "V72: float64\n",
      "V73: float64\n",
      "V74: float64\n",
      "V75: float64\n",
      "V76: float64\n",
      "V77: float64\n",
      "V78: float64\n",
      "V79: float64\n",
      "V80: float64\n",
      "V81: float64\n",
      "V82: float64\n",
      "V83: float64\n",
      "V84: float64\n",
      "V85: float64\n",
      "V86: float64\n",
      "V87: float64\n",
      "V88: float64\n",
      "V89: float64\n",
      "V90: float64\n",
      "V91: float64\n",
      "V92: float64\n",
      "V93: float64\n",
      "V94: float64\n",
      "V95: float64\n",
      "V96: float64\n",
      "V97: float64\n",
      "V98: float64\n",
      "V99: float64\n",
      "V100: float64\n",
      "V101: float64\n",
      "V102: float64\n",
      "V103: float64\n",
      "V104: float64\n",
      "V105: float64\n",
      "V106: float64\n",
      "V107: float64\n",
      "V108: float64\n",
      "V109: float64\n",
      "V110: float64\n",
      "V111: float64\n",
      "V112: float64\n",
      "V113: float64\n",
      "V114: float64\n",
      "V115: float64\n",
      "V116: float64\n",
      "V117: float64\n",
      "V118: float64\n",
      "V119: float64\n",
      "V120: float64\n",
      "V121: float64\n",
      "V122: float64\n",
      "V123: float64\n",
      "V124: float64\n",
      "V125: float64\n",
      "V126: float64\n",
      "V127: float64\n",
      "V128: float64\n",
      "V129: float64\n",
      "V130: float64\n",
      "V131: float64\n",
      "V132: float64\n",
      "V133: float64\n",
      "V134: float64\n",
      "V135: float64\n",
      "V136: float64\n",
      "V137: float64\n",
      "V279: float64\n",
      "V280: float64\n",
      "V281: float64\n",
      "V282: float64\n",
      "V283: float64\n",
      "V284: float64\n",
      "V285: float64\n",
      "V286: float64\n",
      "V287: float64\n",
      "V288: float64\n",
      "V289: float64\n",
      "V290: float64\n",
      "V291: float64\n",
      "V292: float64\n",
      "V293: float64\n",
      "V294: float64\n",
      "V295: float64\n",
      "V296: float64\n",
      "V297: float64\n",
      "V298: float64\n",
      "V299: float64\n",
      "V300: float64\n",
      "V301: float64\n",
      "V302: float64\n",
      "V303: float64\n",
      "V304: float64\n",
      "V305: float64\n",
      "V306: float64\n",
      "V307: float64\n",
      "V308: float64\n",
      "V309: float64\n",
      "V310: float64\n",
      "V311: float64\n",
      "V312: float64\n",
      "V313: float64\n",
      "V314: float64\n",
      "V315: float64\n",
      "V316: float64\n",
      "V317: float64\n",
      "V318: float64\n",
      "V319: float64\n",
      "V320: float64\n",
      "V321: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the data types of each column\n",
    "train_data_types = clean_train_data.dtypes\n",
    "\n",
    "# Get a list of all column names\n",
    "train_column_names = clean_train_data.columns.tolist()\n",
    "\n",
    "# Print the column names and data types\n",
    "for train_column_name, train_data_type in zip(train_column_names, train_data_types):\n",
    "    print(f\"{train_column_name}: {train_data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a17d85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionID: int64\n",
      "TransactionDT: int64\n",
      "TransactionAmt: float64\n",
      "ProductCD: object\n",
      "card1: int64\n",
      "card2: float64\n",
      "card3: float64\n",
      "card4: object\n",
      "card5: float64\n",
      "card6: object\n",
      "addr1: float64\n",
      "addr2: float64\n",
      "P_emaildomain: object\n",
      "C1: float64\n",
      "C2: float64\n",
      "C3: float64\n",
      "C4: float64\n",
      "C5: float64\n",
      "C6: float64\n",
      "C7: float64\n",
      "C8: float64\n",
      "C9: float64\n",
      "C10: float64\n",
      "C11: float64\n",
      "C12: float64\n",
      "C13: float64\n",
      "C14: float64\n",
      "D1: float64\n",
      "D2: float64\n",
      "D3: float64\n",
      "D4: float64\n",
      "D5: float64\n",
      "D10: float64\n",
      "D11: float64\n",
      "D15: float64\n",
      "M1: object\n",
      "M2: object\n",
      "M3: object\n",
      "M4: object\n",
      "M6: object\n",
      "M7: object\n",
      "M8: object\n",
      "M9: object\n",
      "V1: float64\n",
      "V2: float64\n",
      "V3: float64\n",
      "V4: float64\n",
      "V5: float64\n",
      "V6: float64\n",
      "V7: float64\n",
      "V8: float64\n",
      "V9: float64\n",
      "V10: float64\n",
      "V11: float64\n",
      "V12: float64\n",
      "V13: float64\n",
      "V14: float64\n",
      "V15: float64\n",
      "V16: float64\n",
      "V17: float64\n",
      "V18: float64\n",
      "V19: float64\n",
      "V20: float64\n",
      "V21: float64\n",
      "V22: float64\n",
      "V23: float64\n",
      "V24: float64\n",
      "V25: float64\n",
      "V26: float64\n",
      "V27: float64\n",
      "V28: float64\n",
      "V29: float64\n",
      "V30: float64\n",
      "V31: float64\n",
      "V32: float64\n",
      "V33: float64\n",
      "V34: float64\n",
      "V35: float64\n",
      "V36: float64\n",
      "V37: float64\n",
      "V38: float64\n",
      "V39: float64\n",
      "V40: float64\n",
      "V41: float64\n",
      "V42: float64\n",
      "V43: float64\n",
      "V44: float64\n",
      "V45: float64\n",
      "V46: float64\n",
      "V47: float64\n",
      "V48: float64\n",
      "V49: float64\n",
      "V50: float64\n",
      "V51: float64\n",
      "V52: float64\n",
      "V53: float64\n",
      "V54: float64\n",
      "V55: float64\n",
      "V56: float64\n",
      "V57: float64\n",
      "V58: float64\n",
      "V59: float64\n",
      "V60: float64\n",
      "V61: float64\n",
      "V62: float64\n",
      "V63: float64\n",
      "V64: float64\n",
      "V65: float64\n",
      "V66: float64\n",
      "V67: float64\n",
      "V68: float64\n",
      "V69: float64\n",
      "V70: float64\n",
      "V71: float64\n",
      "V72: float64\n",
      "V73: float64\n",
      "V74: float64\n",
      "V75: float64\n",
      "V76: float64\n",
      "V77: float64\n",
      "V78: float64\n",
      "V79: float64\n",
      "V80: float64\n",
      "V81: float64\n",
      "V82: float64\n",
      "V83: float64\n",
      "V84: float64\n",
      "V85: float64\n",
      "V86: float64\n",
      "V87: float64\n",
      "V88: float64\n",
      "V89: float64\n",
      "V90: float64\n",
      "V91: float64\n",
      "V92: float64\n",
      "V93: float64\n",
      "V94: float64\n",
      "V95: float64\n",
      "V96: float64\n",
      "V97: float64\n",
      "V98: float64\n",
      "V99: float64\n",
      "V100: float64\n",
      "V101: float64\n",
      "V102: float64\n",
      "V103: float64\n",
      "V104: float64\n",
      "V105: float64\n",
      "V106: float64\n",
      "V107: float64\n",
      "V108: float64\n",
      "V109: float64\n",
      "V110: float64\n",
      "V111: float64\n",
      "V112: float64\n",
      "V113: float64\n",
      "V114: float64\n",
      "V115: float64\n",
      "V116: float64\n",
      "V117: float64\n",
      "V118: float64\n",
      "V119: float64\n",
      "V120: float64\n",
      "V121: float64\n",
      "V122: float64\n",
      "V123: float64\n",
      "V124: float64\n",
      "V125: float64\n",
      "V126: float64\n",
      "V127: float64\n",
      "V128: float64\n",
      "V129: float64\n",
      "V130: float64\n",
      "V131: float64\n",
      "V132: float64\n",
      "V133: float64\n",
      "V134: float64\n",
      "V135: float64\n",
      "V136: float64\n",
      "V137: float64\n",
      "V279: float64\n",
      "V280: float64\n",
      "V281: float64\n",
      "V282: float64\n",
      "V283: float64\n",
      "V284: float64\n",
      "V285: float64\n",
      "V286: float64\n",
      "V287: float64\n",
      "V288: float64\n",
      "V289: float64\n",
      "V290: float64\n",
      "V291: float64\n",
      "V292: float64\n",
      "V293: float64\n",
      "V294: float64\n",
      "V295: float64\n",
      "V296: float64\n",
      "V297: float64\n",
      "V298: float64\n",
      "V299: float64\n",
      "V300: float64\n",
      "V301: float64\n",
      "V302: float64\n",
      "V303: float64\n",
      "V304: float64\n",
      "V305: float64\n",
      "V306: float64\n",
      "V307: float64\n",
      "V308: float64\n",
      "V309: float64\n",
      "V310: float64\n",
      "V311: float64\n",
      "V312: float64\n",
      "V313: float64\n",
      "V314: float64\n",
      "V315: float64\n",
      "V316: float64\n",
      "V317: float64\n",
      "V318: float64\n",
      "V319: float64\n",
      "V320: float64\n",
      "V321: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the data types of each column\n",
    "test_data_types = clean_test_data.dtypes\n",
    "\n",
    "# Get a list of all column names\n",
    "test_column_names = clean_test_data.columns.tolist()\n",
    "\n",
    "# Print the column names and data types\n",
    "for test_column_name, test_data_type in zip(test_column_names, test_data_types):\n",
    "    print(f\"{test_column_name}: {test_data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0032076c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pause' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4180\\1188747208.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpause\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pause' is not defined"
     ]
    }
   ],
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2afc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform label encoding to convert categorical variables into numerical variables.\n",
    "\n",
    "# create an instance of LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# loop over each column in the clean_train_data dataframe\n",
    "for col in clean_train_data.columns:\n",
    "    # if the column is not numeric\n",
    "    if clean_train_data[col].dtype == 'object':\n",
    "        # fit the LabelEncoder on the column\n",
    "        le.fit(clean_train_data[col])\n",
    "        # transform the column using the LabelEncoder\n",
    "        clean_train_data[col] = le.transform(clean_train_data[col])\n",
    "\n",
    "# loop over each column in the clean_test_data dataframe\n",
    "for col in clean_test_data.columns:\n",
    "    # if the column is not numeric\n",
    "    if clean_test_data[col].dtype == 'object':\n",
    "        # fit the LabelEncoder on the column\n",
    "        le.fit(clean_test_data[col])\n",
    "        # transform the column using the LabelEncoder\n",
    "        clean_test_data[col] = le.transform(clean_test_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d2ffb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "Columns dropped from clean_train_data: []\n",
      "----------------------------------------------------------------------------------\n",
      "Columns dropped from clean_test_data: ['D5', 'M7', 'M8', 'M9']\n"
     ]
    }
   ],
   "source": [
    "# Correcting differing number of features between training and test data sets.\n",
    "\n",
    "# list the columns in each dataset\n",
    "train_cols = clean_train_data.columns.tolist()\n",
    "test_cols = clean_test_data.columns.tolist()\n",
    "\n",
    "# drop columns that aren't in both datasets (except 'isFraud' in train_df)\n",
    "cols_to_drop = [col for col in train_cols if col not in test_cols and col != 'isFraud']\n",
    "train_dropped_cols = [col for col in cols_to_drop if col != 'isFraud']\n",
    "train_common_df = clean_train_data.drop(cols_to_drop, axis=1)\n",
    "test_dropped_cols = [col for col in test_cols if col not in train_cols]\n",
    "test_common_df = clean_test_data.drop([col for col in test_cols if col not in train_cols], axis=1)\n",
    "\n",
    "\n",
    "# list the columns that were dropped from both datasets\n",
    "print('----------------------------------------------------------------------------------')\n",
    "print(\"Columns dropped from clean_train_data:\", train_dropped_cols)\n",
    "print('----------------------------------------------------------------------------------')\n",
    "print(\"Columns dropped from clean_test_data:\", test_dropped_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aaf47e",
   "metadata": {},
   "source": [
    "Good practice to perform feature engineering before scaling the data. Feature engineering involves creating new features or transforming existing ones to better represent the underlying patterns in the data and make it more suitable for modeling.\n",
    "\n",
    "Some common examples of feature engineering include one-hot encoding categorical variables, creating interaction terms between features, and normalizing skewed distributions.\n",
    "\n",
    "Resampling: One common approach is to resample the data to balance the classes. This can be done by either undersampling the majority class, oversampling the minority class, or a combination of both. One way to oversample the minority class is to create synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9c936",
   "metadata": {},
   "source": [
    "#### Test set has no target variable. Therefore the following steps were taken:\n",
    "Separate the target variable from the training set and create a new dataframe that contains only the features that you will use to train the model.\n",
    "\n",
    "Split the training set into a smaller training set and a validation set using the train_test_split() function from scikit-learn. This will allow you to evaluate the performance of your model on a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e018386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target in the train data.\n",
    "X = train_common_df.drop(columns=['isFraud', 'TransactionID'])\n",
    "y = train_common_df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dfca3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X_train, X_val, y_train, y_val\n",
    "# first run test_size = 0.2\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f01d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "392e6f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled and y_val have different number of samples.\n"
     ]
    }
   ],
   "source": [
    "if X_train_scaled.shape[0] == y_val.shape[0]:\n",
    "    print(\"X_train_scaled and y_val have the same number of samples.\")\n",
    "else:\n",
    "    print(\"X_train_scaled and y_val have different number of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e728665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_scaled: (472432, 218)\n",
      "Shape of y_val: (118108,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "986d75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression model.\n",
    "LogReg = LogisticRegression(max_iter=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38fc4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.9691680495817387\n",
      "Testing Data Score: 0.035916280014901616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shann\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    }
   ],
   "source": [
    "# Fit our model by using training data\n",
    "LogReg.fit(X_train_scaled, y_train)\n",
    "print(f\"Training Data Score: {LogReg.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {LogReg.score(X_val, y_val)}\")\n",
    "\n",
    "# When test_size was switched to 0.4\n",
    "# Training Data Score: 0.9691 (96.91%)\n",
    "# Testing Data Score: 0.03528 (3.53%)\n",
    "# Minimmal change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68730f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shann\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "C:\\Users\\shann\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[     0 113866]\n",
      " [     0   4242]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00    113866\n",
      "           1       0.04      1.00      0.07      4242\n",
      "\n",
      "    accuracy                           0.04    118108\n",
      "   macro avg       0.02      0.50      0.03    118108\n",
      "weighted avg       0.00      0.04      0.00    118108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shann\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shann\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the validation set\n",
    "## accuracy = accuracy_score(y_valid, y_pred)\n",
    "## precision = precision_score(y_valid, y_pred)\n",
    "## recall = recall_score(y_valid, y_pred)\n",
    "## f1 = f1_score(y_valid, y_pred)\n",
    "\n",
    "y_pred = LogReg.predict(X_val)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38f1499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the entire training set\n",
    "LogReg.fit(scaler.transform(X), y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_features = test_common_df.drop(columns=['TransactionID'])\n",
    "test_predictions = LogReg.predict(scaler.transform(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1413ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  Predictions\n",
      "0        3663549            0\n",
      "1        3663550            0\n",
      "2        3663551            0\n",
      "3        3663552            0\n",
      "4        3663553            0\n",
      "5        3663554            0\n",
      "6        3663555            0\n",
      "7        3663556            0\n",
      "8        3663557            0\n",
      "9        3663558            0\n",
      "-------------------------------------\n",
      "0    497988\n",
      "1      8703\n",
      "Name: Predictions, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Add the predicted labels to the test_common_df dataframe\n",
    "test_common_df['Predictions'] = test_predictions\n",
    "\n",
    "# Save the predictions to an excel file\n",
    "predictions_df = test_common_df[['TransactionID', 'Predictions']]\n",
    "predictions_df.to_excel('Predictions/test_predictions.xlsx', index=False)\n",
    "\n",
    "# Display the first 10 rows of the predictions dataframe\n",
    "print(predictions_df.head(10))\n",
    "\n",
    "# Count the occurrences of each value in the 'Predictions' column\n",
    "counts = predictions_df['Predictions'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print('-------------------------------------')\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pause. stopped here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbb652",
   "metadata": {},
   "source": [
    "#### Optimisation Methods for logistic regressions\n",
    "\n",
    "1) Regularisation methods like L1 and L2 can help to reduce overfitting and improve the generalization performance of the model.\n",
    "\n",
    "2) Address missing data in categorical variables by using mode imputation.\n",
    "\n",
    "3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b71696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f01e179b",
   "metadata": {},
   "source": [
    "## Data Imbalance Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd99e1",
   "metadata": {},
   "source": [
    "In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n",
    "\n",
    "The data is broken into two files identity and transaction, which are joined by TransactionID.\n",
    "\n",
    "Note: Not all transactions have corresponding identity information.\n",
    "\n",
    "##### Categorical Features - Transaction\n",
    "\n",
    "ProductCD\n",
    "emaildomain\n",
    "card1 - card6\n",
    "addr1, addr2\n",
    "P_emaildomain\n",
    "R_emaildomain\n",
    "M1 - M9\n",
    "\n",
    "##### Categorical Features - Identity\n",
    "\n",
    "DeviceType\n",
    "DeviceInfo\n",
    "id_12 - id_38\n",
    "\n",
    "The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).\n",
    "\n",
    "#### Files\n",
    "\n",
    "train_{transaction, identity}.csv - the training set\n",
    "test_{transaction, identity}.csv - the test set (you must predict the isFraud value for these observations)\n",
    "sample_submission.csv - a sample submission file in the correct format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44371caa",
   "metadata": {},
   "source": [
    "###### Focus on train_df\n",
    "Not all transactions have corresponding identity information.\n",
    "\n",
    "Considerable imbalance in dataset: Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df['isFraud'].value_counts().index\n",
    "y = train_df['isFraud'].value_counts().values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x, y, alpha=0.8)\n",
    "\n",
    "ax.set_title('Data imbalance - isFraud')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('isFraud')\n",
    "ax.set_xticks(x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9958af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Device Type\n",
    "ax = sns.countplot(x=\"DeviceType\", data=train_df)\n",
    "ax.set_title('DeviceType', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c180f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Unique Devices = \",train_df['DeviceInfo'].nunique())\n",
    "train_df['DeviceInfo'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e466ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\n",
    "for i in cards:\n",
    "    print (\"Unique \",i, \" = \",train_df[i].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud by card type\n",
    "fig, ax = plt.subplots(1, 4, figsize=(25,5))\n",
    "\n",
    "sns.countplot(x=\"card4\", ax=ax[0], data=train_df.loc[train_df['isFraud'] == 0])\n",
    "ax[0].set_title('card4 isFraud=0', fontsize=14)\n",
    "sns.countplot(x=\"card4\", ax=ax[1], data=train_df.loc[train_df['isFraud'] == 1])\n",
    "ax[1].set_title('card4 isFraud=1', fontsize=14)\n",
    "sns.countplot(x=\"card6\", ax=ax[2], data=train_df.loc[train_df['isFraud'] == 0])\n",
    "ax[2].set_title('card6 isFraud=0', fontsize=14)\n",
    "sns.countplot(x=\"card6\", ax=ax[3], data=train_df.loc[train_df['isFraud'] == 1])\n",
    "ax[3].set_title('card6 isFraud=1', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb10632c",
   "metadata": {},
   "source": [
    "### Solving class imbalancing by resampling. \n",
    "\n",
    "Undersample majority class as we have a ton of data considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748176d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_df['isFraud'].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(train_df.drop('isFraud',axis=1),y_train,test_size=.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e451ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.concat([X_train,y_train],axis=1)\n",
    "\n",
    "\n",
    "not_fraud=X[X.isFraud==0]\n",
    "fraud=X[X.isFraud==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_fraud_downsampled = resample(not_fraud,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(fraud), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, fraud])\n",
    "\n",
    "# checking counts\n",
    "downsampled.isFraud.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=downsampled.isFraud.value_counts()\n",
    "sns.barplot(y=y,x=[0,1])\n",
    "plt.title('downsampled data class count')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address missing values\n",
    "# drop columns with more than 50% missing values in train\n",
    "mis_val = downsampled.isnull().sum()/len(downsampled)\n",
    "downsampled.drop(columns = mis_val[mis_val > 0.5].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# filter numerical data\n",
    "num_mv_train = downsampled.select_dtypes(include=np.number)\n",
    "\n",
    "# filter categorical data\n",
    "cat_mv_train = downsampled.select_dtypes(exclude=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "num_df_train = pd.DataFrame(median.fit_transform(num_mv_train), columns=num_mv_train.columns)\n",
    "max = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "cat_df_train = pd.DataFrame(max.fit_transform(cat_mv_train), columns=cat_mv_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34228bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_train = pd.concat([num_df_train, cat_df_train], axis=1)\n",
    "print(comb_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab985475",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_v2 = comb_train['isFraud']\n",
    "features_v2 = comb_train.drop(columns=['isFraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for target_v2\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(target_v2)\n",
    "encoded_df = enc.transform(target_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for features_v2\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(features_v2)\n",
    "encoded_df = enc.transform(features_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6dc864",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_valid2, y_train2, y_valid2 = train_test_split(features_v2, target_v2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53736e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardise the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled2 = scaler.fit_transform(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aed90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model and print the model score.\n",
    "\n",
    "# Instantiate a logistic regression model.\n",
    "LogReg = LogisticRegression()\n",
    "LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59af5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg.fit(X_train_scaled2, y_valid2)\n",
    "print(f\"Training Data Score: {LogReg.score(X_train_scaled2, y_train2)}\")\n",
    "print(f\"Testing Data Score: {LogReg.score(X_valid2, y_valid2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LogReg.predict(X_valid2)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_valid2, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_valid2, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d9794",
   "metadata": {},
   "source": [
    "### Address Missing Values"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
   "id": "4efa7f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbc8cc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(test_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2eeec114",
=======
   "execution_count": null,
   "id": "e5324d4a",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_clean = train_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
   "id": "f4e5fbac",
=======
   "execution_count": null,
   "id": "644e188c",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_clean = test_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
   "id": "4b92226e",
=======
   "execution_count": null,
   "id": "1383cc1e",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Shapes\n",
    "print(\"Train Dataset shape: \", train_df_clean.shape)\n",
    "print(\"Test Dataset shape: \", test_df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "e724f500",
=======
   "id": "ec4a304c",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "source": [
    "##### all of the columns have some missing values\n",
    "\n",
    "Thus if we drop all rows that have missing data we're going to lose the whole dataset."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
   "id": "bd8c264d",
=======
   "execution_count": null,
   "id": "8694bd2d",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df\n",
    "column_train_df = train_df.columns.tolist()\n",
    "print(column_train_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
   "id": "cc4dbba7",
=======
   "execution_count": null,
   "id": "4cf8d709",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display options to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
   "id": "b75e326e",
=======
   "execution_count": null,
   "id": "a6221599",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "e74099e6",
=======
   "id": "3101a3cd",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "source": [
    "#### Check number of columns with more than 50% (and 60%) missing values. Arbitrary cut-off at the moment. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
   "id": "c03d9d58",
=======
   "execution_count": null,
   "id": "918e703c",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data60(df) :\n",
    "    count = df.isnull().sum()\n",
    "    percent = (df.isnull().sum()) / (df.isnull().count()) * 100\n",
    "    total = pd.concat([count, percent], axis=1, keys = ['Count', 'Percent'])\n",
    "    types = []\n",
    "    for col in df.columns :\n",
    "        dtypes = str(df[col].dtype)\n",
    "        types.append(dtypes)\n",
    "    total['dtypes'] = types\n",
    "    \n",
    "    # count number of columns with more than 60% missing values\n",
    "    num_missing_over_60 = (total['Percent'] > 60).sum()\n",
    "    print(f\"Number of columns with more than 60% missing values: {num_missing_over_60}\")\n",
    "    \n",
    "    return np.transpose(total)\n",
    "\n",
    "missing_data60(train_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
   "id": "211d0f3e",
=======
   "execution_count": null,
   "id": "5b301fe0",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data60(test_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
   "id": "dc4fe61c",
=======
   "execution_count": null,
   "id": "16a7a85a",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess whether the columns with more than 60% missing data are the same in both datasets.\n",
    "\n",
    "def assess_missing_data60(df1, df2):\n",
    "    # calculate missing data for first dataframe\n",
    "    count1 = df1.isnull().sum()\n",
    "    percent1 = (df1.isnull().sum()) / (df1.isnull().count()) * 100\n",
    "    total1 = pd.concat([count1, percent1], axis=1, keys=['Count', 'Percent'])\n",
    "\n",
    "    # calculate missing data for second dataframe\n",
    "    count2 = df2.isnull().sum()\n",
    "    percent2 = (df2.isnull().sum()) / (df2.isnull().count()) * 100\n",
    "    total2 = pd.concat([count2, percent2], axis=1, keys=['Count', 'Percent'])\n",
    "\n",
    "    # identify columns with more than 60% missing values in first dataframe\n",
    "    cols_to_drop1 = total1[total1['Percent'] > 60].index.tolist()\n",
    "\n",
    "    # identify columns with more than 60% missing values in second dataframe\n",
    "    cols_to_drop2 = total2[total2['Percent'] > 60].index.tolist()\n",
    "\n",
    "    # check if columns with more than 60% missing values are the same in both dataframes\n",
    "    if set(cols_to_drop1) == set(cols_to_drop2):\n",
    "        print(\"Both dataframes have the same columns with more than 60% missing values.\")\n",
    "    else:\n",
    "        print(\"Both dataframes have different columns with more than 60% missing values.\")\n",
    "        print(\"Columns to drop in first dataframe:\", cols_to_drop1)\n",
    "        print(\"Columns to drop in second dataframe:\", cols_to_drop2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
   "id": "ca832e57",
=======
   "execution_count": null,
   "id": "1fa1e599",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_missing_data60(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
   "id": "542955c2",
=======
   "execution_count": null,
   "id": "74e3e26d",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data50(df) :\n",
    "    count = df.isnull().sum()\n",
    "    percent = (df.isnull().sum()) / (df.isnull().count()) * 100\n",
    "    total = pd.concat([count, percent], axis=1, keys = ['Count', 'Percent'])\n",
    "    types = []\n",
    "    for col in df.columns :\n",
    "        dtypes = str(df[col].dtype)\n",
    "        types.append(dtypes)\n",
    "    total['dtypes'] = types\n",
    "    \n",
    "    # count number of columns with more than 50% missing values\n",
    "    num_missing_over_50 = (total['Percent'] > 50).sum()\n",
    "    print(f\"Number of columns with more than 50% missing values: {num_missing_over_50}\")\n",
    "    \n",
    "    return np.transpose(total)\n",
    "\n",
    "missing_data50(train_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
   "id": "517c45b6",
=======
   "execution_count": null,
   "id": "6138434d",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data50(test_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
   "id": "8e705520",
=======
   "execution_count": null,
   "id": "0652f1c5",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess whether the columns with more than 50% missing data are the same.\n",
    "\n",
    "def assess_missing_data50(df1, df2):\n",
    "    # calculate missing data for first dataframe\n",
    "    count1 = df1.isnull().sum()\n",
    "    percent1 = (df1.isnull().sum()) / (df1.isnull().count()) * 100\n",
    "    total1 = pd.concat([count1, percent1], axis=1, keys=['Count', 'Percent'])\n",
    "\n",
    "    # calculate missing data for second dataframe\n",
    "    count2 = df2.isnull().sum()\n",
    "    percent2 = (df2.isnull().sum()) / (df2.isnull().count()) * 100\n",
    "    total2 = pd.concat([count2, percent2], axis=1, keys=['Count', 'Percent'])\n",
    "\n",
    "    # identify columns with more than 50% missing values in first dataframe\n",
    "    cols_to_drop1 = total1[total1['Percent'] > 50].index.tolist()\n",
    "\n",
    "    # identify columns with more than 50% missing values in second dataframe\n",
    "    cols_to_drop2 = total2[total2['Percent'] > 50].index.tolist()\n",
    "\n",
    "    # check if columns with more than 50% missing values are the same in both dataframes\n",
    "    if set(cols_to_drop1) == set(cols_to_drop2):\n",
    "        print(\"Both dataframes have the same columns with more than 50% missing values.\")\n",
    "    else:\n",
    "        print(\"Both dataframes have different columns with more than 50% missing values.\")\n",
    "        print(\"Columns to drop in first dataframe:\", cols_to_drop1)\n",
    "        print(\"Columns to drop in second dataframe:\", cols_to_drop2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 45,
   "id": "f46cf33f",
=======
   "execution_count": null,
   "id": "862f1724",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_missing_data50(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 46,
   "id": "1bed9ee1",
=======
   "execution_count": null,
   "id": "39815a7f",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision taken to delete columns with more than 60% missing values\n",
    "\n",
    "def reduced_missing60(df) :\n",
    "    count = df.isnull().sum()\n",
    "    percent = (df.isnull().sum()) / (df.isnull().count()) * 100\n",
    "    total = pd.concat([count, percent], axis=1, keys = ['Count', 'Percent'])\n",
    "    types = []\n",
    "    for col in df.columns :\n",
    "        dtypes = str(df[col].dtype)\n",
    "        types.append(dtypes)\n",
    "    total['dtypes'] = types\n",
    "    \n",
    "    # count number of columns with more than 60% missing values\n",
    "    num_missing_over_60 = (total['Percent'] > 60).sum()\n",
    "    print(f\"Dropped {num_missing_over_60} columns with more than 60% missing values.\")\n",
    "    \n",
    "    # select columns with more than 60% missing values\n",
    "    cols_to_drop = total[total['Percent'] > 60].index.tolist()\n",
    "    \n",
    "    # drop columns with more than 60% missing values\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
   "id": "34552973",
=======
   "execution_count": null,
   "id": "08e75c10",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_train = reduced_missing60(train_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
   "id": "7337c5a4",
=======
   "execution_count": null,
   "id": "8ae3ae3a",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_test = reduced_missing60(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fcf75",
   "metadata": {},
   "source": [
    "#### Identify columns in each dataframe that contain categorical data."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 49,
   "id": "0cbba78e",
=======
   "execution_count": null,
   "id": "54379e65",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduced_train\n",
    "# find categorical columns\n",
    "cat_cols_train = [col for col in reduced_train.columns if reduced_train[col].dtype == 'object']\n",
    "\n",
    "# print categorical column names\n",
    "print(cat_cols_train)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 50,
   "id": "edb37f01",
=======
   "execution_count": null,
   "id": "d8f78c99",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduced_test\n",
    "# find categorical columns\n",
    "cat_cols_test = [col for col in reduced_test.columns if reduced_test[col].dtype == 'object']\n",
    "\n",
    "# print categorical column names\n",
    "print(cat_cols_test)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
   "id": "ccfe3ca7",
=======
   "execution_count": null,
   "id": "d91e7660",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable values\n",
    "# test_df has no 'isFraud' variable.\n",
    "reduced_train[\"isFraud\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1557374",
   "metadata": {},
   "source": [
    "#### Identify and remove outliers"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 52,
   "id": "557aaf51",
=======
   "execution_count": null,
   "id": "9b658e7f",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "### TBC"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "f700c92c",
=======
   "id": "04e1456d",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "source": [
    "### Visualise variables\n",
    "\n",
    "Create scatter plot for each feature."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
   "id": "26c4fcbf",
=======
   "execution_count": null,
   "id": "0ecffc76",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "### TBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad1ba7",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "1) Convert categorical variables into numerical data.\n",
    "\n",
    "2) Scale the data to ensure that all the features are on a similar scale.\n",
    "> Fit pre-processor to training data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 54,
   "id": "d400e8db",
=======
   "execution_count": null,
   "id": "59a0e4a2",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X (features) and y (target) sets.\n",
    "# Separate the target variable from the training set.\n",
    "# Create a new dataframe that contains only the features that you will use to train the model.\n",
    "\n",
    "target = reduced_train['isFraud']\n",
    "features = reduced_train.drop(columns=['isFraud'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 55,
   "id": "268b0418",
=======
   "execution_count": null,
   "id": "5c958f2d",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran into some trouble with running a label encoding.\n",
    "# The error is a KeyError which occurred when attempting to access the key 'M5' in the features and reduced_test dataframes. \n",
    "# This indicates that the key 'M5' is not present in either dataframe.\n",
    "# reduced_train has an additional column of categorical data (M5)\n",
    "# count the values for each category in the 'M5' columns\n",
    "count = reduced_train['M5'].value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 56,
   "id": "28047745",
=======
   "execution_count": null,
   "id": "3457f620",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reduced_train['M5'])"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "3b308521",
=======
   "id": "2f13e809",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "source": [
    "##### Apply label encoding to convert categorical variables into numerical variables.\n",
    "\n",
    "Used label encoding instead of one-hot encoding as it would create even more features.\n",
    "Label Encoding for categorical variables in both features and reduced_test.\n",
    "\n",
    "To account for the reduced_test dataframe having an additional categorical variable that is not present in the features dataframe, we can modify the code to include an additional check for whether a column is present in both dataframes before performing the LabelEncoder transformation. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 57,
   "id": "15c22b5e",
=======
   "execution_count": null,
   "id": "eed9a102",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performs label encoding on both datasets\n",
    "\n",
    "# for f in features.columns:\n",
    "#     if features[f].dtype == 'object' or reduced_test[f].dtype == 'object':\n",
    "#         if f in reduced_test.columns:\n",
    "#             lbl = LabelEncoder()\n",
    "#             lbl.fit(list(features[f].values) + list(reduced_test[f].values))\n",
    "#             features[f] = lbl.transform(list(features[f].values))\n",
    "#             reduced_test[f] = lbl.transform(list(reduced_test[f].values))\n",
    "#         else:\n",
    "#             lbl = LabelEncoder()\n",
    "#             lbl.fit(list(features[f].values))\n",
    "#             features[f] = lbl.transform(list(features[f].values))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 67,
   "id": "0be36463",
=======
   "execution_count": null,
   "id": "9fec7b58",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just label encoding on features from reduced_train\n",
    "# create an instance of LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# loop over each column in the dataframe\n",
    "for col in features.columns:\n",
    "    # if the column is not numeric\n",
    "    if features[col].dtype == 'object':\n",
    "        # fit the LabelEncoder on the column\n",
    "        le.fit(features[col])\n",
    "        # transform the column using the LabelEncoder\n",
    "        features[col] = le.transform(features[col])"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "26f9308f",
=======
   "id": "68aec690",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "source": [
    "##### Test set has no target variable. Therefore the following steps were taken:\n",
    "Separate the target variable from the training set and create a new dataframe that contains only the features that you will use to train the model.\n",
    "\n",
    "Split the training set into a smaller training set and a validation set using the train_test_split() function from scikit-learn. This will allow you to evaluate the performance of your model on a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 68,
   "id": "2aba300b",
=======
   "execution_count": null,
   "id": "842451ee",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 69,
   "id": "397b0612",
=======
   "execution_count": null,
   "id": "e8891244",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardise the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caceef88",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Select the most relevant features that will be used to train the model. This can be done by analyzing the correlation between the different features and selecting the ones that have the most impact on the output."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 70,
   "id": "fcb053dd",
=======
   "execution_count": null,
   "id": "59ab0c5c",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently heaps of features, may need to conduct feature selection\n",
    "# see 19.3 (04-Ins_Forest-Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e000f77",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "i) Instantiate a logistic regression model using the LogisticRegression() function from scikit-learn.\n",
    "\n",
    "ii) Train the logistic regression model using the smaller training set by calling the fit() function on the model object and passing in the training feature and target variables.\n",
    "\n",
    "iii) Evaluate the performance of the model on the validation set by calling the predict() function on the model object and passing in the validation features. You can use classification metrics such as accuracy, precision, recall, and F1 score to evaluate the performance of the model.\n",
    "\n",
    "iii) Once you are satisfied with the performance of the model on the validation set, you can train the model on the entire training set by calling the fit() function on the model object and passing in the entire training feature and target variables.\n",
    "\n",
    "iv) Finally, you can use the trained model to make predictions on the test set by calling the predict() function on the model object and passing in the test features."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 71,
   "id": "b84389f7",
=======
   "execution_count": null,
   "id": "125472c6",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model and print the model score.\n",
    "\n",
    "# Instantiate a logistic regression model.\n",
    "LogReg = LogisticRegression()\n",
    "LogReg"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 72,
   "id": "a4b92294",
=======
   "execution_count": null,
   "id": "7d5371ac",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(X_train_scaled).sum())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 73,
   "id": "a433adc3",
=======
   "execution_count": null,
   "id": "2bf78720",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(y_valid).sum())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 74,
   "id": "b20c633b",
=======
   "execution_count": null,
   "id": "200249d6",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error suggests that there might be some missing or infinite values in the input data X_train_scaled and y_valid.\n",
    "# Attempt to resolve using a simple imputer.\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 75,
   "id": "7289e50b",
=======
   "execution_count": null,
   "id": "e96281a8",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_imputed = imputer.fit_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 76,
   "id": "66c415cd",
=======
   "execution_count": null,
   "id": "6977d859",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model by using training data\n",
    "LogReg.fit(X_train_scaled_imputed, y_valid)\n",
    "print(f\"Training Data Score: {LogReg.score(X_train_scaled_imputed, y_train)}\")\n",
    "print(f\"Testing Data Score: {LogReg.score(X_valid, y_valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "6389855d",
=======
   "id": "fe5d509d",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "source": [
    "### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "8a640ff0",
=======
   "id": "d51ee6cd",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model on the validation set\n",
    "## accuracy = accuracy_score(y_valid, y_pred)\n",
    "## precision = precision_score(y_valid, y_pred)\n",
    "## recall = recall_score(y_valid, y_pred)\n",
    "## f1 = f1_score(y_valid, y_pred)\n",
    "\n",
    "y_pred = LogReg.predict(X_valid)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_valid, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_valid, y_pred))"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "id": "208d46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892d874",
=======
   "cell_type": "markdown",
   "id": "a98efe2f",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "source": [
    "### Train the model on the entire training dataset and make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "f5a191a3",
=======
   "id": "831ed3aa",
>>>>>>> 986c8504886f78a21000d7faddff723bcbfd48a8
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the entire training set\n",
    "LogReg.fit(features, target)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_features = reduced_test\n",
    "test_predictions = LogReg.predict(test_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
